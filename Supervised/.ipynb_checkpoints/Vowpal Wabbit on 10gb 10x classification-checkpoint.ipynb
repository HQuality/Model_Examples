{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как в действительности Vowpal Wabbit справляется с большими выборками. Имеются 10 Гб вопросов со StackOverflow – ссылка на данные, там аккурат 10 миллионов вопросов, и у каждого вопроса может быть несколько тегов.\n",
    "\n",
    "\n",
    "Из всех тегов выделены 10, и решается задача классификации на 10 классов: по тексту вопроса надо поставить один из 10 тегов, соответствующих 10 популярным языкам программирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999992 stackoverflow.10kk.tsv\n",
      "CPU times: user 63.3 ms, sys: 10.9 ms, total: 74.2 ms\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -i 1,7d stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999992it [01:10, 140992.95it/s]\n",
      "4389052 lines selected, 15 lines corrupted.\n",
      "CPU times: user 1.64 s, sys: 475 ms, total: 2.11 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 preprocess.py stackoverflow.10kk.tsv stackoverflow.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 | i ve got some code in window scroll that checks if an element is visible then triggers another function however only the first section of code is firing both bits of code work in and of themselves if i swap their order whichever is on top fires correctly my code is as follows fn isonscreen function use strict var win window viewport top win scrolltop left win scrollleft bounds this offset viewport right viewport left + win width viewport bottom viewport top + win height bounds right bounds left + this outerwidth bounds bottom bounds top + this outerheight return viewport right lt bounds left viewport left gt bounds right viewport bottom lt bounds top viewport top gt bounds bottom window scroll function use strict var load_more_results ajax load_more_results isonscreen if load_more_results true loadmoreresults var load_more_staff ajax load_more_staff isonscreen if load_more_staff true loadmorestaff what am i doing wrong can you only fire one event from window scroll i assume not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: vw: not found\r\n"
     ]
    }
   ],
   "source": [
    "!vw -d \"stackoverflow_train.vw\" \\\n",
    "--oaa 10 \\\n",
    "--bit_precision 28 \\\n",
    "--random_seed 17  \\\n",
    "--passes 1 \\\n",
    "--ngram 1 \\\n",
    "--loss_function hinge \\\n",
    "-f \"model_passes_1_1.vw\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-oaa 10 – указываем, что классификация на 10 классов\n",
    "-d – путь к данным\n",
    "-f – путь к модели, которая будет построена\n",
    "-b 28 – используем 28 бит для хэширования, то есть признаковое пространство ограничено  228  признаками, что в данном случае больше, чем число уникальных слов в выборке (но потом появятся би- и триграммы, и ограничение размерности признакового пространства начнет работать)\n",
    "также указываем random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i model_passes_1_1.vw -t -d stackoverflow_valid.vw \\\n",
    "-p stackoverflow_valid_pred.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1 0.9166626794748937\n"
     ]
    }
   ],
   "source": [
    "with open('stackoverflow_valid_pred.txt') as pred_file, open('stackoverflow_valid_labels.txt') as valid_labels:\n",
    "    print (\"1-1\",accuracy_score(np.loadtxt(valid_labels), np.loadtxt(pred_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = itertools.product([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (1,3,5)\n",
    "n = (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = model_passes_1_ngram_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = model_passes_1_ngram_1.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        7        7      209\n",
      "0.781250 0.812500           32           32.0        7        2      174\n",
      "0.765625 0.750000           64           64.0        3        3      204\n",
      "0.648438 0.531250          128          128.0        1        5       29\n",
      "0.609375 0.570312          256          256.0        5        1      169\n",
      "0.548828 0.488281          512          512.0        2        2      303\n",
      "0.456055 0.363281         1024         1024.0        3        3      123\n",
      "0.375000 0.293945         2048         2048.0        1        5       83\n",
      "0.309082 0.243164         4096         4096.0        1        1       79\n",
      "0.261841 0.214600         8192         8192.0        2        2      112\n",
      "0.220825 0.179810        16384        16384.0        7        7      252\n",
      "0.186096 0.151367        32768        32768.0        4        5      134\n",
      "0.159149 0.132202        65536        65536.0        5        5      145\n",
      "0.138329 0.117508       131072       131072.0        7        2      255\n",
      "0.120888 0.103447       262144       262144.0        7        7      101\n",
      "0.108549 0.096210       524288       524288.0        1        1      818\n",
      "0.099817 0.091085      1048576      1048576.0        1        1      571\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.095764\n",
      "total feature number = 291954690\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = model_passes_1_ngram_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = model_passes_1_ngram_2.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.081809\n",
      "total feature number = 580983344\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = model_passes_1_ngram_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = model_passes_1_ngram_3.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        7        7      622\n",
      "0.781250 0.812500           32           32.0        7        2      517\n",
      "0.750000 0.718750           64           64.0        3        3      607\n",
      "0.648438 0.546875          128          128.0        1        7       82\n",
      "0.628906 0.609375          256          256.0        5        1      502\n",
      "0.564453 0.500000          512          512.0        2        2      904\n",
      "0.471680 0.378906         1024         1024.0        3        3      364\n",
      "0.400879 0.330078         2048         2048.0        1        5      244\n",
      "0.333252 0.265625         4096         4096.0        1        1      232\n",
      "0.281006 0.228760         8192         8192.0        2        2      331\n",
      "0.232361 0.183716        16384        16384.0        7        7      751\n",
      "0.192169 0.151978        32768        32768.0        4        5      397\n",
      "0.160690 0.129211        65536        65536.0        5        5      430\n",
      "0.136086 0.111481       131072       131072.0        7        2      760\n",
      "0.115993 0.095901       262144       262144.0        7        7      298\n",
      "0.102705 0.089417       524288       524288.0        1        1     2449\n",
      "0.090337 0.077969      1048576      1048576.0        1        1     1708\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.085885\n",
      "total feature number = 868548985\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = model_passes_3_ngram_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = model_passes_3_ngram_1.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.750000 0.750000           32           32.0        1        7      404\n",
      "0.703125 0.656250           64           64.0        7        7      103\n",
      "0.617188 0.531250          128          128.0        5        5      276\n",
      "0.593750 0.570312          256          256.0        1        1      102\n",
      "0.533203 0.472656          512          512.0        2        5       68\n",
      "0.457031 0.380859         1024         1024.0        1        1      132\n",
      "0.383301 0.309570         2048         2048.0        7        7       71\n",
      "0.310059 0.236816         4096         4096.0        2        2      319\n",
      "0.262085 0.214111         8192         8192.0        5        5       24\n",
      "0.223450 0.184814        16384        16384.0        3        3      581\n",
      "0.185547 0.147644        32768        32768.0        3        3       28\n",
      "0.158493 0.131439        65536        65536.0        4        4      184\n",
      "0.137115 0.115738       131072       131072.0        2        2       95\n",
      "0.121109 0.105103       262144       262144.0        5        5      232\n",
      "0.108404 0.095699       524288       524288.0        6        6      142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100007 0.091610      1048576      1048576.0        1        1      422\n",
      "0.092582 0.092582      2097152      2097152.0        5        5      696 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.084107 h\n",
      "total feature number = 788274150\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = model_passes_3_ngram_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = model_passes_3_ngram_2.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        2        7      316\n",
      "0.718750 0.687500           32           32.0        1        7      806\n",
      "0.703125 0.687500           64           64.0        7        7      204\n",
      "0.632812 0.562500          128          128.0        5        5      550\n",
      "0.609375 0.585938          256          256.0        1        1      202\n",
      "0.531250 0.453125          512          512.0        2        5      134\n",
      "0.458984 0.386719         1024         1024.0        1        1      262\n",
      "0.382812 0.306641         2048         2048.0        7        7      140\n",
      "0.305664 0.228516         4096         4096.0        2        2      636\n",
      "0.255249 0.204834         8192         8192.0        5        7       46\n",
      "0.215027 0.174805        16384        16384.0        3        3     1160\n",
      "0.177612 0.140198        32768        32768.0        3        3       54\n",
      "0.148590 0.119568        65536        65536.0        4        4      366\n",
      "0.127281 0.105972       131072       131072.0        2        2      188\n",
      "0.110416 0.093552       262144       262144.0        5        5      462\n",
      "0.096605 0.082794       524288       524288.0        6        6      282\n",
      "0.086320 0.076035      1048576      1048576.0        1        1      842\n",
      "0.077806 0.077806      2097152      2097152.0        5        5     1390 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.069835 h\n",
      "total feature number = 1568647998\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = model_passes_3_ngram_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = model_passes_3_ngram_3.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.718750 0.687500           32           32.0        1        7     1207\n",
      "0.687500 0.656250           64           64.0        7        7      304\n",
      "0.640625 0.593750          128          128.0        5        1      823\n",
      "0.625000 0.609375          256          256.0        1        1      301\n",
      "0.544922 0.464844          512          512.0        2        1      199\n",
      "0.466797 0.388672         1024         1024.0        1        1      391\n",
      "0.402344 0.337891         2048         2048.0        7        7      208\n",
      "0.328125 0.253906         4096         4096.0        2        2      952\n",
      "0.279419 0.230713         8192         8192.0        5        2       67\n",
      "0.232727 0.186035        16384        16384.0        3        3     1738\n",
      "0.191132 0.149536        32768        32768.0        3        3       79\n",
      "0.159424 0.127716        65536        65536.0        4        4      547\n",
      "0.135880 0.112335       131072       131072.0        2        2      280\n",
      "0.117374 0.098869       262144       262144.0        5        5      691\n",
      "0.102125 0.086876       524288       524288.0        6        6      421\n",
      "0.090308 0.078491      1048576      1048576.0        1        1     1261\n",
      "0.081231 0.081231      2097152      2097152.0        5        5     2083 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073110 h\n",
      "total feature number = 2345071710\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = model_passes_5_ngram_1.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = model_passes_5_ngram_1.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.750000 0.750000           32           32.0        1        7      404\n",
      "0.703125 0.656250           64           64.0        7        7      103\n",
      "0.617188 0.531250          128          128.0        5        5      276\n",
      "0.593750 0.570312          256          256.0        1        1      102\n",
      "0.533203 0.472656          512          512.0        2        5       68\n",
      "0.457031 0.380859         1024         1024.0        1        1      132\n",
      "0.383301 0.309570         2048         2048.0        7        7       71\n",
      "0.310059 0.236816         4096         4096.0        2        2      319\n",
      "0.262085 0.214111         8192         8192.0        5        5       24\n",
      "0.223450 0.184814        16384        16384.0        3        3      581\n",
      "0.185547 0.147644        32768        32768.0        3        3       28\n",
      "0.158493 0.131439        65536        65536.0        4        4      184\n",
      "0.137115 0.115738       131072       131072.0        2        2       95\n",
      "0.121109 0.105103       262144       262144.0        5        5      232\n",
      "0.108404 0.095699       524288       524288.0        6        6      142\n",
      "0.100007 0.091610      1048576      1048576.0        1        1      422\n",
      "0.092582 0.092582      2097152      2097152.0        5        5      696 h\n",
      "0.088155 0.083728      4194304      4194304.0        1        1      216 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083287 h\n",
      "total feature number = 1313790250\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = model_passes_5_ngram_2.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = model_passes_5_ngram_2.vw.cache\n",
      "Reading datafile = stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        2        7      316\n",
      "0.718750 0.687500           32           32.0        1        7      806\n",
      "0.703125 0.687500           64           64.0        7        7      204\n",
      "0.632812 0.562500          128          128.0        5        5      550\n",
      "0.609375 0.585938          256          256.0        1        1      202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531250 0.453125          512          512.0        2        5      134\n",
      "0.458984 0.386719         1024         1024.0        1        1      262\n",
      "0.382812 0.306641         2048         2048.0        7        7      140\n",
      "0.305664 0.228516         4096         4096.0        2        2      636\n",
      "0.255249 0.204834         8192         8192.0        5        7       46\n",
      "0.215027 0.174805        16384        16384.0        3        3     1160\n",
      "0.177612 0.140198        32768        32768.0        3        3       54\n",
      "0.148590 0.119568        65536        65536.0        4        4      366\n",
      "0.127281 0.105972       131072       131072.0        2        2      188\n",
      "0.110416 0.093552       262144       262144.0        5        5      462\n",
      "0.096605 0.082794       524288       524288.0        6        6      282\n",
      "0.086320 0.076035      1048576      1048576.0        1        1      842\n",
      "0.077806 0.077806      2097152      2097152.0        5        5     1390 h\n",
      "0.073817 0.069828      4194304      4194304.0        1        1      430 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.069835 h\n",
      "total feature number = 2614413330\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = model_passes_5_ngram_3.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = model_passes_5_ngram_3.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.718750 0.687500           32           32.0        1        7     1207\n",
      "0.687500 0.656250           64           64.0        7        7      304\n",
      "0.640625 0.593750          128          128.0        5        1      823\n",
      "0.625000 0.609375          256          256.0        1        1      301\n",
      "0.544922 0.464844          512          512.0        2        1      199\n",
      "0.466797 0.388672         1024         1024.0        1        1      391\n",
      "0.402344 0.337891         2048         2048.0        7        7      208\n",
      "0.328125 0.253906         4096         4096.0        2        2      952\n",
      "0.279419 0.230713         8192         8192.0        5        2       67\n",
      "0.232727 0.186035        16384        16384.0        3        3     1738\n",
      "0.191132 0.149536        32768        32768.0        3        3       79\n",
      "0.159424 0.127716        65536        65536.0        4        4      547\n",
      "0.135880 0.112335       131072       131072.0        2        2      280\n",
      "0.117374 0.098869       262144       262144.0        5        5      691\n",
      "0.102125 0.086876       524288       524288.0        6        6      421\n",
      "0.090308 0.078491      1048576      1048576.0        1        1     1261\n",
      "0.081231 0.081231      2097152      2097152.0        5        5     2083 h\n",
      "0.077415 0.073600      4194304      4194304.0        1        1      643 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073110 h\n",
      "total feature number = 3908452850\n"
     ]
    }
   ],
   "source": [
    "for passes,ngram in [(passes,ngram) for passes in p for ngram in n]:\n",
    "    !vw -d \"stackoverflow_train.vw\" \\\n",
    "    --oaa 10 \\\n",
    "    --bit_precision 28 \\\n",
    "    --random_seed 17  \\\n",
    "    --cache_file \"model_passes_{passes}_ngram_{ngram}.vw.cache\" \\\n",
    "    --passes $passes \\\n",
    "    --ngram $ngram \\\n",
    "    --loss_function hinge \\\n",
    "    -f \"model_passes_{passes}_ngram_{ngram}.vw\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = np.loadtxt('stackoverflow_valid_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i model_passes_1_ngram_1.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_11.txt --quiet\n",
    "!vw -i model_passes_1_ngram_2.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_12.txt --quiet\n",
    "!vw -i model_passes_1_ngram_3.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_13.txt --quiet\n",
    "!vw -i model_passes_3_ngram_1.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_31.txt --quiet\n",
    "!vw -i model_passes_3_ngram_2.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_32.txt --quiet\n",
    "!vw -i model_passes_3_ngram_3.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_33.txt --quiet\n",
    "!vw -i model_passes_5_ngram_1.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_51.txt --quiet\n",
    "!vw -i model_passes_5_ngram_2.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_52.txt --quiet\n",
    "!vw -i model_passes_5_ngram_3.vw -t -d stackoverflow_valid.vw -p stackoverflow_valid_pred_53.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passes_1_ngram_1:  0.9166626794748937\n",
      "passes_1_ngram_2:  0.932356949811964\n",
      "passes_1_ngram_3:  0.9299701028968885\n",
      "passes_3_ngram_1:  0.9180078440593349\n",
      "passes_3_ngram_2:  0.9305374233263022\n",
      "passes_3_ngram_3:  0.9272544835401888\n",
      "passes_5_ngram_1:  0.9180269825798453\n",
      "passes_5_ngram_2:  0.93123529580634\n",
      "passes_5_ngram_3:  0.9281464752996887\n"
     ]
    }
   ],
   "source": [
    "    print(\"passes_1_ngram_1: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_11.txt'))))\n",
    "    print(\"passes_1_ngram_2: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_12.txt'))))\n",
    "    print(\"passes_1_ngram_3: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_13.txt'))))\n",
    "    print(\"passes_3_ngram_1: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_31.txt'))))\n",
    "    print(\"passes_3_ngram_2: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_32.txt'))))\n",
    "    print(\"passes_3_ngram_3: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_33.txt'))))\n",
    "    print(\"passes_5_ngram_1: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_51.txt'))))\n",
    "    print(\"passes_5_ngram_2: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_52.txt'))))\n",
    "    print(\"passes_5_ngram_3: \", (accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_53.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = accuracy_score(valid_labels, np.loadtxt('stackoverflow_valid_pred_12.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i model_passes_1_ngram_2.vw -t -d stackoverflow_test.vw -p stackoverflow_test_pred_12.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.loadtxt('stackoverflow_test_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination on test set: 0.9325346646452743\n"
     ]
    }
   ],
   "source": [
    "acc2 = accuracy_score(test_labels, np.loadtxt('stackoverflow_test_pred_12.txt'))\n",
    "print (\"Best combination on test set:\", acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017771483331030513"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acc2-acc1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = model_merged.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = stackoverflow_merged.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "0.077170 0.068335      2097152      2097152.0        2        2      342\n",
      "\n",
      "finished run\n",
      "number of examples = 2926036\n",
      "weighted example sum = 2926036.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073888\n",
      "total feature number = 1161593772\n"
     ]
    }
   ],
   "source": [
    "    !vw -d \"stackoverflow_merged.vw\" \\\n",
    "    --oaa 10 \\\n",
    "    --bit_precision 28 \\\n",
    "    --random_seed 17  \\\n",
    "    --passes 1 \\\n",
    "    --ngram 2 \\\n",
    "    --loss_function hinge \\\n",
    "    -f \"model_merged.vw\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i model_merged.vw -t -d stackoverflow_test.vw -p stackoverflow_test_pred_merged.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination on test set: 0.9364471250524601 Acc gain: 0.3912460407185736\n"
     ]
    }
   ],
   "source": [
    "acc_merged = accuracy_score(test_labels, np.loadtxt('stackoverflow_test_pred_merged.txt'))\n",
    "print (\"Best combination on test set:\", acc_merged, \"Acc gain:\", (acc_merged-acc2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удивительно, как простая модель VW может обучиться на такой выборке за секунды или минуты на простом железе, без всяких Hadoop-кластеров. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
